{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ASL_Alphabets_Classification_Rev.01.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1Xuul064q5PeU3VCM0T2_8-UfEP_ch7G6",
      "authorship_tag": "ABX9TyO3x7d3o2pVTtKXvih4PMiv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bhairavsingh/ASL_Alphabets_Detection/blob/master/ASL_Alphabets_Classification_Rev_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfI1IR8l59Gu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj8IFPMj5_xB"
      },
      "source": [
        "\n",
        "#Loading required libraries.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "from pprint import pprint\n",
        "from collections import Counter\n",
        "import keras\n",
        "import os\n",
        "import json\n",
        "import sklearn\n",
        "import csv\n",
        "\n",
        "import tensorflow as tf\n",
        "#!pip install tensorflow-gpu\n",
        "#import tensorflow-gpu as tfg\n",
        "\n",
        "from keras.datasets import imdb\n",
        "from keras import preprocessing\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras import regularizers\n",
        "from keras.layers import Embedding, Flatten, Dense, Dropout\n",
        "from keras import layers\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import LeakyReLU, UpSampling1D, Input, Reshape, Activation, Lambda, AveragePooling1D,AveragePooling2D\n",
        "from keras.layers import Convolution2D, Dense, MaxPooling2D, Flatten, BatchNormalization, Dropout, Conv2DTranspose\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import image\n",
        "#!pip install pypng\n",
        "#import png\n",
        "#!pip install Pillow\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from glob import glob\n",
        "\n",
        "from numpy import save\n",
        "from numpy import load\n",
        "import time\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3B3KyOW5_ze"
      },
      "source": [
        "\n",
        "\n",
        "#Function for reading image paths and creating labels of images based on the folder names it is stored in.\n",
        "def image_listing(directory):\n",
        "  #Listing folder names from given directory.\n",
        "  directory = directory + '/*/'\n",
        "  Directory = glob(directory)\n",
        "  File_names = []\n",
        "  #Creating list of list of images stored in each folder of directory.\n",
        "  for i in range(len(Directory)):\n",
        "    Folders = Directory[i] + '*'\n",
        "    File_names.append(glob(Folders))\n",
        "\n",
        "  #Listing all file path in one list and creating a list of labels.\n",
        "  file_path = []\n",
        "  labels = []\n",
        "  for i in range(len(File_names)):\n",
        "    for j in range(len(File_names[i])):\n",
        "      #Reading each file path.\n",
        "      x = File_names[i][j]\n",
        "      file_path.append(x)\n",
        "      #Reading folder name where image is stored for labeling it.\n",
        "      if x[62:65] == 'del':\n",
        "        labels.append(x[62:65])\n",
        "      elif x[62:67] == 'space':\n",
        "        labels.append(x[62:67])\n",
        "      elif x[62:69] == 'nothing':\n",
        "        labels.append(x[62:69])\n",
        "      else:\n",
        "        labels.append(x[62])\n",
        "  return file_path, labels\n",
        "\n",
        "\n",
        "\n",
        "#Function for creating train, validation and testsets according to the given percentage values.\n",
        "def set_creator(directory, trainset_percentage = 0.8, validationset_percentage = 0.1, testset_percentage = 0.1):\n",
        "  #Reading path and labels of all images.\n",
        "  file_names, labels = image_listing(directory)\n",
        "\n",
        "  #Image names meta data.\n",
        "  Data_A = file_names[15008 : 18007]\n",
        "  Data_B = file_names[21008 : 24015]\n",
        "  Data_C = file_names[27016 : 30023]\n",
        "  Data_D = file_names[0 : 2999]\n",
        "  Data_E = file_names[12008 : 15007]\n",
        "  Data_F = file_names[18008 : 21007]\n",
        "  Data_G = file_names[6000 : 9007]\n",
        "  Data_H = file_names[24016 : 27015]\n",
        "  Data_I = file_names[9008 : 12007]\n",
        "  Data_J = file_names[33024 : 36023]\n",
        "  Data_K = file_names[51024 : 54031]\n",
        "  Data_L = file_names[45024 : 48023]\n",
        "  Data_M = file_names[42024 : 45023]\n",
        "  Data_N = file_names[57032 : 60031]\n",
        "  Data_O = file_names[48024 : 51023]\n",
        "  Data_P = file_names[54032 : 57031]\n",
        "  Data_Q = file_names[30024 : 33023]\n",
        "  Data_R = file_names[36024 : 39023]\n",
        "  Data_S = file_names[63032 : 66031]\n",
        "  Data_T = file_names[81032 : 84031]\n",
        "  Data_U = file_names[66032 : 69031]\n",
        "  Data_V = file_names[72032 : 75031]\n",
        "  Data_W = file_names[84032 :]\n",
        "  Data_X = file_names[78032 : 81031]\n",
        "  Data_Y = file_names[60032 : 63031]\n",
        "  Data_Z = file_names[75032 : 78031]\n",
        "  Data_nothing = file_names[39024 : 42023]\n",
        "  Data_del = file_names[3000 : 5999]\n",
        "  Data_space = file_names[69032 : 72031]\n",
        "\n",
        "  #Labels metadata.\n",
        "  Labels_A = labels[15008 : 18007]\n",
        "  Labels_B = labels[21008 : 24015]\n",
        "  Labels_C = labels[27016 : 30023]\n",
        "  Labels_D = labels[0 : 2999]\n",
        "  Labels_E = labels[12008 : 15007]\n",
        "  Labels_F = labels[18008 : 21007]\n",
        "  Labels_G = labels[6000 : 9007]\n",
        "  Labels_H = labels[24016 : 27015]\n",
        "  Labels_I = labels[9008 : 12007]\n",
        "  Labels_J = labels[33024 : 36023]\n",
        "  Labels_K = labels[51024 : 54031]\n",
        "  Labels_L = labels[45024 : 48023]\n",
        "  Labels_M = labels[42024 : 45023]\n",
        "  Labels_N = labels[57032 : 60031]\n",
        "  Labels_O = labels[48024 : 51023]\n",
        "  Labels_P = labels[54032 : 57031]\n",
        "  Labels_Q = labels[30024 : 33023]\n",
        "  Labels_R = labels[36024 : 39023]\n",
        "  Labels_S = labels[63032 : 66031]\n",
        "  Labels_T = labels[81032 : 84031]\n",
        "  Labels_U = labels[66032 : 69031]\n",
        "  Labels_V = labels[72032 : 75031]\n",
        "  Labels_W = labels[84032 :]\n",
        "  Labels_X = labels[78032 : 81031]\n",
        "  Labels_Y = labels[60032 : 63031]\n",
        "  Labels_Z = labels[75032 : 78031]\n",
        "  Labels_nothing = labels[39024 : 42023]\n",
        "  Labels_del = labels[3000 : 5999]\n",
        "  Labels_space = labels[69032 : 72031]\n",
        "\n",
        "  Data_file_meta_info = [Data_A, Data_B, Data_C, Data_D, Data_E, Data_F, Data_G, Data_H, Data_I, Data_J, Data_K, Data_L, Data_M, Data_N, Data_O, \n",
        "                        Data_P, Data_Q, Data_R, Data_S, Data_T, Data_U, Data_V, Data_W, Data_X, Data_Y, Data_Z, Data_nothing, Data_del, Data_space]\n",
        "\n",
        "  Label_file_meta_info = [Labels_A, Labels_B, Labels_C, Labels_D, Labels_E, Labels_F, Labels_G, Labels_H, Labels_I, Labels_J, Labels_K, Labels_L, Labels_M, Labels_N, Labels_O, \n",
        "                          Labels_P, Labels_Q, Labels_R, Labels_S, Labels_T, Labels_U, Labels_V, Labels_W, Labels_X, Labels_Y, Labels_Z, Labels_nothing, Labels_del, Labels_space]\n",
        "\n",
        "  #Declaring lists for all sets.\n",
        "  train_file_names = []\n",
        "  train_labels = []\n",
        "  val_file_names = []\n",
        "  val_labels = []\n",
        "  test_file_names = []\n",
        "  test_labels = []\n",
        "  \n",
        "  #Loop for equally extracting images and respective labels from each class.\n",
        "  for i in range(len(Data_file_meta_info)):\n",
        "    #Training value and validation value.\n",
        "    train_value = int(len(Data_file_meta_info[i]) * trainset_percentage)\n",
        "    val_value = train_value + int(len(Data_file_meta_info[i]) * validationset_percentage)\n",
        "    #Train sets.\n",
        "    train_file_names.extend(Data_file_meta_info[i][ : train_value])\n",
        "    train_labels.extend(Label_file_meta_info[i][ : train_value])\n",
        "    #Validation sets.\n",
        "    val_file_names.extend(Data_file_meta_info[i][train_value : val_value])\n",
        "    val_labels.extend(Label_file_meta_info[i][train_value : val_value])\n",
        "    #Test sets.\n",
        "    test_file_names.extend(Data_file_meta_info[i][val_value : ])\n",
        "    test_labels.extend(Label_file_meta_info[i][val_value : ])\n",
        "  return train_file_names, train_labels, val_file_names, val_labels, test_file_names, test_labels\n",
        "\n",
        "\n",
        "\n",
        "#Function for reading the images.\n",
        "def image_reader(image_path, normalizing_value = 255):\n",
        "  #Reading image.\n",
        "  data_raw = image.imread(image_path)\n",
        "  #print(data_raw.dtype)\n",
        "  #print(data_raw.shape)\n",
        "  plt.imshow(data_raw)\n",
        "  plt.show()\n",
        "  #Normalizing the image array using max pixel value.\n",
        "  normalized_data = np.array(data_raw/normalizing_value)\n",
        "  return normalized_data\n",
        "\n",
        "\n",
        "\n",
        "#Function for reading list of paths and reading images to get arrays.\n",
        "def read_image_get_array(file_paths, normalizing_value = 255):\n",
        "  lol = []\n",
        "  for i in range(len(file_paths)):\n",
        "    x = image_reader(file_paths[i], normalizing_value)\n",
        "    lol.append(x)\n",
        "  arrays = np.array(lol)\n",
        "  return arrays\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "#Reading all file path names and respective labels.\n",
        "file_names, labels = image_listing(directory = '/content/drive/My Drive/Colab_Datasets/ASL/asl_alphabet_train')\n",
        "\n",
        "#Writing the lyrics in a csv file!\n",
        "#file = open('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Image_paths.csv','w+', encoding = 'utf-8', newline = '\\n')\n",
        "file = open('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Image_labels.csv','w+', encoding = 'utf-8', newline = '\\n')\n",
        "with file:\n",
        "    objects = csv.writer(file)\n",
        "    objects.writerows(labels)\n",
        "file.close()\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "#Index dictonary for classes and its key integer values.\n",
        "meta_dict_classes = {'nothing' : 0, 'A' : 1, 'B' : 2, 'C' : 3, 'D' : 4, 'E' : 5, 'F' : 6, 'G' : 7, 'H' : 8, 'I' : 9, 'J' : 10, 'K' : 11, 'L' : 12, 'M' : 13, \n",
        "                       'N' : 14, 'O' : 15, 'P' : 16, 'Q' : 17, 'R' : 18, 'S' : 19, 'T' : 20, 'U' : 21, 'V' : 22, 'W' : 23, 'X' : 24, 'Y' : 25, 'Z' : 26, 'del' : 27, 'space' : 28}\n",
        "#meta_pred_dict_classes = {value:key for key, value in meta_dict_classes.items()}\n",
        "meta_pred_dict_classes = {0: 'nothing', 1: 'A', 2: 'B', 3: 'C', 4: 'D', 5: 'E', 6: 'F', 7: 'G', 8: 'H', 9: 'I', 10: 'J', 11: 'K', 12: 'L', 13: 'M', \n",
        "                          14: 'N', 15: 'O', 16: 'P', 17: 'Q', 18: 'R', 19: 'S', 20: 'T', 21: 'U', 22: 'V', 23: 'W', 24: 'X', 25: 'Y', 26: 'Z', 27: 'del', 28: 'space'}\n",
        "\n",
        "\n",
        "#Function for converting alphabets to key integers and converting to categorical array (one hot encodding).\n",
        "def converting_to_intkeys(label_list, meta_dict_classes = meta_dict_classes):\n",
        "  for n, i in enumerate(label_list):\n",
        "    label_list[n] = meta_dict_classes[i]\n",
        "  label_array = tf.keras.utils.to_categorical(label_list)\n",
        "  return label_array\n",
        "\n",
        "\n",
        "#Function for predicting given image using the trained model.\n",
        "def predict_image(test_path, model, meta_pred_dict_classes = meta_pred_dict_classes):\n",
        "  #Reading image and generating normalized numpy array.\n",
        "  test_image = read_image_get_array(file_paths = [test_path], normalizing_value = 255)\n",
        "  #Displaying image.\n",
        "  plt.imshow(image.imread(test_path))\n",
        "  plt.show()\n",
        "  #Predicting class.\n",
        "  result = model.predict(test_image)\n",
        "  #Class integer key.\n",
        "  predicted_class_key = np.argmax(result)\n",
        "  #print(predicted_class_key)\n",
        "  #Finding classes based on predefined dictonary.\n",
        "  predicted_class = meta_pred_dict_classes[predicted_class_key]\n",
        "  return predicted_class\n",
        "\n",
        "\n",
        "#Function for predicting class of given image using class name and image number from database using trained model.\n",
        "def customized_predict(class_name, image_number, model):\n",
        "  test_path = '/content/drive/My Drive/Colab_Datasets/ASL/asl_alphabet_train/' + class_name + '/' + class_name + image_number + '.jpg'\n",
        "  predicted_class = predict_image(test_path = test_path, model = model)\n",
        "  print(\"Predicted class of above image is: \", predicted_class)\n",
        "  return predicted_class\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZQMMQy-5_tD",
        "outputId": "32344b99-102d-45f0-cd5b-585e8c019989",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "\n",
        "\n",
        "#Loading training sets.\n",
        "training_array_1 = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/training_array_1.npy', allow_pickle = True)\n",
        "training_array_2 = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/training_array_2.npy', allow_pickle = True)\n",
        "training_array_3 = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/training_array_3.npy', allow_pickle = True)\n",
        "training_array_4 = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/training_array_4.npy', allow_pickle = True)\n",
        "training_array_5 = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/training_array_5.npy', allow_pickle = True)\n",
        "#training_array_6 = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/training_array_6.npy', allow_pickle = True)\n",
        "#training_array_7 = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/training_array_7.npy', allow_pickle = True)\n",
        "#training_array_8 = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/training_array_8.npy', allow_pickle = True)\n",
        "#training_array_9 = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/training_array_9.npy', allow_pickle = True)\n",
        "training_array_10 = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/training_array_10.npy', allow_pickle = True)\n",
        "\n",
        "training_array_11 = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/training_array_11.npy', allow_pickle = True)\n",
        "training_array_12 = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/training_array_12.npy', allow_pickle = True)\n",
        "training_array_13 = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/training_array_13.npy', allow_pickle = True)\n",
        "training_array_14 = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/training_array_14.npy', allow_pickle = True)\n",
        "training_array_15 = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/training_array_15.npy', allow_pickle = True)\n",
        "training_array_16 = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/training_array_16.npy', allow_pickle = True)\n",
        "training_array_17 = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/training_array_17.npy', allow_pickle = True)\n",
        "training_array_18 = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/training_array_18.npy', allow_pickle = True)\n",
        "training_array_19 = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/training_array_19.npy', allow_pickle = True)\n",
        "training_array_20 = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/training_array_20.npy', allow_pickle = True)\n",
        "\n",
        "training_array_21 = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/training_array_21.npy', allow_pickle = True)\n",
        "training_array_22 = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/training_array_22.npy', allow_pickle = True)\n",
        "training_array_23 = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/training_array_23.npy', allow_pickle = True)\n",
        "training_array_24 = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/training_array_24.npy', allow_pickle = True)\n",
        "training_array_25 = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/training_array_25.npy', allow_pickle = True)\n",
        "training_array_26 = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/training_array_26.npy', allow_pickle = True)\n",
        "training_array_27 = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/training_array_27.npy', allow_pickle = True)\n",
        "training_array_28 = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/training_array_28.npy', allow_pickle = True)\n",
        "training_array_29 = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/training_array_29.npy', allow_pickle = True)\n",
        "\n",
        "'''\n",
        "#Concatenating all training arrays in one array.\n",
        "training_array = np.concatenate((training_array_1, training_array_2, training_array_3, training_array_4, training_array_5, \n",
        "                                 training_array_6, training_array_7, training_array_8, training_array_9, training_array_10,\n",
        "                                 training_array_11, training_array_12, training_array_13, training_array_14, training_array_15, \n",
        "                                 training_array_16, training_array_17, training_array_18, training_array_19, training_array_20,\n",
        "                                 training_array_21, training_array_22, training_array_23, training_array_24, training_array_25, \n",
        "                                 training_array_26, training_array_27, training_array_28, training_array_29), axis = 0)\n",
        "print(\"Shape of training set: \", training_array.shape)\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n#Concatenating all training arrays in one array.\\ntraining_array = np.concatenate((training_array_1, training_array_2, training_array_3, training_array_4, training_array_5, \\n                                 training_array_6, training_array_7, training_array_8, training_array_9, training_array_10,\\n                                 training_array_11, training_array_12, training_array_13, training_array_14, training_array_15, \\n                                 training_array_16, training_array_17, training_array_18, training_array_19, training_array_20,\\n                                 training_array_21, training_array_22, training_array_23, training_array_24, training_array_25, \\n                                 training_array_26, training_array_27, training_array_28, training_array_29), axis = 0)\\nprint(\"Shape of training set: \", training_array.shape)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5f9UvH45_r7"
      },
      "source": [
        "\n",
        "#Loading validation sets.\n",
        "validation_array = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/validation_array.npy', allow_pickle = True)\n",
        "\n",
        "#Loading label sets.\n",
        "y_train = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/y_train.npy', allow_pickle = True)\n",
        "y_val = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/y_val.npy', allow_pickle = True)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br6h1-Ue5_nQ",
        "outputId": "ed504dee-03b9-4823-ca0e-8028cdb72f72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "training_array_11.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(500, 200, 200, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AT2eytgp5_kp"
      },
      "source": [
        "\n",
        "training_array_19 = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/training_array_19.npy', allow_pickle = True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RXmTdj4DjN5",
        "outputId": "eb331068-32a8-4e76-dc1c-16290b40ecc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        }
      },
      "source": [
        "\n",
        "#training_array_7 = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/training_array_7.npy', allow_pickle = True)\n",
        "#training_array_8 = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/training_array_8.npy', allow_pickle = True)\n",
        "training_array_9 = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/training_array_9.npy', allow_pickle = True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-0de59bccaa6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#training_array_7 = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/training_array_7.npy', allow_pickle = True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#training_array_8 = np.load('/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/training_array_8.npy', allow_pickle = True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtraining_array_9\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/Colab_Datasets/ASL/ASL_Alphabet_Numpy_Data/training_array_9.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0;32m--> 453\u001b[0;31m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    454\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    783\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m             \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 16383984 into shape (500,200,200,3)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60lj7zizDjUs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxUlU_twDjaq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6bwKZj5Djd3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9orRgMkDj0E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrsanU49DjZU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcKkQa9yDjTs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
